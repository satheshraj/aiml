{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14329390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from scikit-learn->sentence-transformers==4.1.0) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers==4.1.0 | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48849c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3396ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "documents = [\n",
    "    'Bugs introduced by the intern had to be squashed by the lead developer.',\n",
    "    'Bugs found by the quality assurance engineer were difficult to debug.',\n",
    "    'Bugs are common throughout the warm summer months, according to the entomologist.',\n",
    "    'Bugs, in particular spiders, are extensively studied by arachnologists.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fe68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f9b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de417d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318c8a5",
   "metadata": {},
   "source": [
    "# Manual implementation of L2(Euclidean) distance calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6071e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_fn(vector1, vector2):\n",
    "    squared_sum = sum((x - y) ** 2 for x, y in zip(vector1, vector2))\n",
    "    return math.sqrt(squared_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbe8eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.96179017134276"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance_fn(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc2c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.96179017134276"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance_fn(embeddings[1], embeddings[0])\n",
    "# Vector positioning for arguments to function does not matter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbe2bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22804333 -0.246477   -0.00319237 -0.4011369   0.58040327 -0.07540709\n",
      " -0.1585729   0.24768439 -0.15160525  0.01536422  0.00947509  0.4250167\n",
      "  0.18076804 -0.04996739 -0.17622906  0.2744977  -0.632379   -0.08400841\n",
      " -0.01563674  0.3895156  -0.4088379  -0.07366164  0.21534097 -0.3809455\n",
      " -0.03634035  0.38411656  0.16714825 -0.5633113  -0.03045717 -0.15912974\n",
      "  0.28667477  0.27080914  0.49389985 -0.06697895  0.15861714 -0.0722226\n",
      " -0.27289623 -0.07229342  0.15310524 -0.103544   -0.20727664  0.4120065\n",
      "  0.04627789 -0.24674499 -0.15725222 -0.52524614 -0.40528253 -0.09747362\n",
      " -0.06160371 -0.32968023  0.31149215 -0.03460407 -0.22514832  0.04832362\n",
      " -0.04793397 -0.05118244  0.30874708  0.36401907 -0.14748739  0.64699197\n",
      "  0.12886815 -0.05396593 -0.0100518   0.14999984  0.15292728 -0.27913466\n",
      " -0.07968055 -0.4148341   0.11531564 -0.20606102 -0.06508962 -0.40328267\n",
      " -0.5096472   0.065418    0.48177364 -0.06020098  0.07172662  0.3426946\n",
      "  0.4951734  -0.07550937 -0.1714772  -0.17710747  0.0199184   0.1485436\n",
      " -0.47351322  0.29483756  0.1765672  -0.0379515   0.3521413   0.3043675\n",
      "  0.41022453 -0.34785226  0.158823    0.38192087 -0.4469309   0.06875014\n",
      "  0.10325457 -0.14234446 -0.33322302  0.17264967 -0.10274803 -0.11864395\n",
      " -0.2951248   0.07747851 -0.10551992 -0.32535878  0.6902669  -0.50488484\n",
      "  0.15584816  0.23706779 -0.10250117 -0.07557248  0.43297324 -0.02076665\n",
      "  0.11082119 -0.2779961   0.22761962 -0.29035228 -0.54770184 -0.08490907\n",
      "  0.78342944  0.12415155  0.32263458 -0.0034566   0.10972986  0.20541656\n",
      " -0.2849095   0.08026348 -0.09285501 -0.08583549 -0.13420875  0.36763495\n",
      "  0.08990979 -0.31420955  0.03231264  0.6937867  -0.08258899 -0.28312135\n",
      " -0.4352666  -0.3876048  -0.21444249  0.19276649  0.3006728   0.11290827\n",
      " -0.1107593   0.10451555  0.124345    0.12442231 -0.3478043   0.12425869\n",
      " -0.12256923 -0.21857248  0.00412142  0.32562235 -0.04699034 -0.22559094\n",
      "  0.55758727  0.26749834 -0.19296676 -0.235008   -0.24596253  0.28287718\n",
      "  0.03147303  0.05992246 -0.24600159 -0.13219376 -0.2755577   0.254646\n",
      "  0.21991065  0.5012192  -0.01458754 -0.11907237  0.45310542 -0.15796503\n",
      "  0.17150317  0.01665021  0.13461307  0.32431012  0.08487523  0.30525753\n",
      "  0.37788177  0.13222787  0.11145389  0.29314113 -0.01669911 -0.42611134\n",
      "  0.14599498  0.19156994  0.0698792   0.11591984 -0.47614715  0.37443942\n",
      "  0.3167103  -0.09457254  0.11973482  0.18008871  0.02910125  0.27650604\n",
      " -0.14371644 -0.18265161 -0.04614653  0.04737788 -0.6939504  -0.23831429\n",
      " -0.08421484  0.46629363 -0.20070569 -0.32656166  0.5397624  -0.23706529\n",
      "  0.13949983 -0.7507948  -0.6763979   0.0258163  -0.10267037  0.22365734\n",
      "  0.06807148  0.39918995  0.15666482  0.15562929 -0.24556121  0.1078431\n",
      " -0.13820642 -0.24121821 -0.8531237  -0.64235693 -0.4808796  -0.33056504\n",
      " -0.22197524  0.20870851 -0.58204114  0.36113164  0.10932783 -0.06032209\n",
      " -0.20961492  0.20166983 -0.02369534  0.12583713  0.01696829  0.10977238\n",
      "  0.2964063  -0.24368308 -0.25924858  0.35230222 -0.18689314  0.7121671\n",
      " -0.24045321 -0.10707526  0.23375165  0.07157681  0.15913677  0.41586065\n",
      " -0.15568462  0.22796838  0.6288981  -0.19906268 -0.25325042  0.26640344\n",
      "  0.5352326   0.30077603 -0.1928964  -0.19895929 -0.43307844 -0.4154924\n",
      "  0.10519972 -0.027775   -0.4629037  -0.13508497  0.27837917 -0.12572284\n",
      " -0.22171941 -0.05882914  0.15336406  0.23059712  0.21981499 -0.18530346\n",
      "  0.11196344 -0.3986073  -0.47100347 -0.14608127  0.3247457   0.17965077\n",
      "  0.6361554  -0.11995916 -0.08106039 -0.01165058  0.06121343 -0.8993642\n",
      " -0.25020385 -0.40626758 -0.118999    0.43138906 -0.19883502 -0.00988814\n",
      "  0.11284642  0.70382214 -0.02291055 -0.07330801 -0.37544063  0.2936504\n",
      " -0.25981036 -0.25757137 -0.6737246  -0.6163705  -0.3305102   0.2725565\n",
      " -0.06514623  0.09026     0.05731194 -0.16438252 -0.06676175  0.03902364\n",
      " -0.32572427 -0.142276    0.02428933 -0.14186609 -0.06512878  0.2595999\n",
      " -0.1243716   0.26002643  0.03753006 -0.01107588  0.01469009  0.4300717\n",
      "  0.6530955  -0.1576473  -0.35865808  0.43707836  0.19136025 -0.13357903\n",
      " -0.32428917 -0.04768202  0.17581134  0.6550434   0.4575121   0.19765402\n",
      " -0.2720696   0.26839828 -0.4794141  -0.3541271  -0.01766792 -0.01321904\n",
      " -0.16114148 -0.15681875  0.30001268 -0.1980493  -0.08497025  0.32020336\n",
      "  0.35698116  0.02857314 -0.08484903  0.12441573  0.4036448  -0.06624579\n",
      "  0.18442452  0.23330657  0.42072096  0.00604088  0.32256457  0.14344162\n",
      " -0.31639412 -0.260723   -0.01333697  0.16483085 -0.51826215 -0.13682419\n",
      " -0.19892046 -0.18935655  0.3482298  -0.08212332  0.25203633 -0.22381543\n",
      " -0.28267995  0.31594032  0.2625428  -0.47609666  0.08095507 -0.1497465\n",
      " -0.27481273  0.03248158 -0.80392593  0.45528123  0.6341974   0.5375048 ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "424334c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179017, 7.33939885, 7.15578169],\n",
       "       [5.96179017, 0.        , 7.76861748, 7.393591  ],\n",
       "       [7.33939885, 7.76861748, 0.        , 5.919928  ],\n",
       "       [7.15578169, 7.393591  , 5.919928  , 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        l2_dist_manual[i,j] = euclidean_distance_fn(embeddings[j], embeddings[i])\n",
    "\n",
    "l2_dist_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d354a81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179017, 7.33939885, 7.15578169],\n",
       "       [5.96179017, 0.        , 7.76861748, 7.393591  ],\n",
       "       [7.33939885, 7.76861748, 0.        , 5.919928  ],\n",
       "       [7.15578169, 7.393591  , 5.919928  , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        l2_dist_manual[i,j] = euclidean_distance_fn(embeddings[i], embeddings[j])\n",
    "\n",
    "l2_dist_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908a7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_dist_manual is a 4×4 array where each element represents the L2 distance between two vectors: the vector at the row index and the vector at the column index. For example, the distance between the first vector (index 0) and the second vector (index 1) is located at position [0, 1] in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef03931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.96179017134276)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a711705c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.96179017134276)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "890933ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Make the manual calculation more efficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae2ad50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code used to populate the `l2_dist_manual array` is not very efficient. First, it redundantly calculates the distance between a vector and itself, even though the L2 distance in such cases is always zero. Second, the array is symmetric—meaning the distance between vectors at indices $i$ and $j$ is the same as between $j$ and $i$. Therefore, each distance only needs to be computed once.\n",
    "\n",
    "#In the cell below, write an improved version of the code that avoids these inefficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65180dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179017, 7.33939885, 7.15578169],\n",
       "       [5.96179017, 0.        , 7.76861748, 7.393591  ],\n",
       "       [7.33939885, 7.76861748, 0.        , 5.919928  ],\n",
       "       [7.15578169, 7.393591  , 5.919928  , 0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_dist_manual_improved = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        if j > i: # Calculate the upper triangle only\n",
    "            l2_dist_manual_improved[i,j] = euclidean_distance_fn(embeddings[i], embeddings[j])\n",
    "        elif i > j: # Copy the uper triangle to the lower triangle\n",
    "            l2_dist_manual_improved[i,j] = l2_dist_manual[j,i]\n",
    "\n",
    "l2_dist_manual_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2e8917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.96179018, 7.33940023, 7.15578141],\n",
       "       [5.96179018, 0.        , 7.76861766, 7.39359074],\n",
       "       [7.33940023, 7.76861766, 0.        , 5.91992798],\n",
       "       [7.15578141, 7.39359074, 5.91992798, 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate L2 distance using numpy\n",
    "l2_dist_scipy = scipy.spatial.distance.cdist(embeddings, embeddings, 'euclidean')\n",
    "l2_dist_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0003924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(l2_dist_manual, l2_dist_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8146dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpret the L2 Distance Results\n",
    "#An analysis of l2_dist_scipy shows that, in this case, the L2 distance metric performed well for similarity search. For example, the first vector—corresponding to the sentence Bugs introduced by the intern had to be squashed by the lead developer.—had the smallest distance to the second vector, which represents the sentence Bugs found by the quality assurance engineer were difficult to debug. This result aligns with expectations, as both sentences refer to programming bugs.\n",
    "\n",
    "#Similarly, the third and fourth sentences—both related to physical bugs rather than programming—were closest to each other in terms of distance, which again matches our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ecbbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dot Product Similarity and Distance\n",
    "# Manual implementation of dot product calculation\n",
    "def dot_product_fn(vector1, vector2):\n",
    "    return sum(x * y for x, y in zip(vector1, vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa6d6aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(18.535393)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_fn(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e408d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.74440002, 18.53539276,  8.56981659,  7.83093262],\n",
       "       [18.53539276, 38.86931992,  7.88995934,  8.66340637],\n",
       "       [ 8.56981659,  7.88995934, 37.2620163 , 17.66956329],\n",
       "       [ 7.83093262,  8.66340637, 17.66956329, 33.12266541]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_manual = np.empty([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        dot_product_manual[i,j] = dot_product_fn(embeddings[i], embeddings[j])\n",
    "\n",
    "dot_product_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46232605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the dot product using matrix multiplication\n",
    "#We can compute the dot product efficiently using the matrix multiplication operator @. To do this, we multiply the embeddings matrix by its transpose. Since embeddings has a shape of 4×384, its transpose will be 384×4. Multiplying these gives us a 4×4 matrix, which is the desired result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a6d101d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.744415 , 18.5354   ,  8.569814 ,  7.8309345],\n",
       "       [18.5354   , 38.86933  ,  7.889962 ,  8.663405 ],\n",
       "       [ 8.569814 ,  7.889962 , 37.262016 , 17.669561 ],\n",
       "       [ 7.8309345,  8.663405 , 17.669561 , 33.122658 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication operator\n",
    "dot_product_operator = embeddings @ embeddings.T\n",
    "dot_product_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1fc4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can verify that the matrix multiplication operator returns the same result as our custom function after accounting for rounding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdf86fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(dot_product_manual, dot_product_operator, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38b9f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3278031269.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mEquivalently, if both of the matrices we want to multiply are two-dimensional, we can use the matmul() function from numpy instead:\u001b[39m\n                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Equivalently, if both of the matrices we want to multiply are two-dimensional, we can use the matmul() function from numpy instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a31094a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.744415 , 18.5354   ,  8.569814 ,  7.8309345],\n",
       "       [18.5354   , 38.86933  ,  7.889962 ,  8.663405 ],\n",
       "       [ 8.569814 ,  7.889962 , 37.262016 , 17.669561 ],\n",
       "       [ 7.8309345,  8.663405 , 17.669561 , 33.122658 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Equivalent to `np.matmul()` if both arrays are 2-D:\n",
    "np.matmul(embeddings,embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "027f4ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.744415 , 18.5354   ,  8.569814 ,  7.8309345],\n",
       "       [18.5354   , 38.86933  ,  7.889962 ,  8.663405 ],\n",
       "       [ 8.569814 ,  7.889962 , 37.262016 , 17.669561 ],\n",
       "       [ 7.8309345,  8.663405 , 17.669561 , 33.122658 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `np.dot` returns an identical result, but `np.matmul` is recommended if both arrays are 2-D:\n",
    "np.dot(embeddings,embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce235803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate dot product distance\n",
    "# The dot product between two vectors provides a similarity score. If, on the other hand, we would like a distance, we can simply take the negative of the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2cbe086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-33.74440002, -18.53539276,  -8.56981659,  -7.83093262],\n",
       "       [-18.53539276, -38.86931992,  -7.88995934,  -8.66340637],\n",
       "       [ -8.56981659,  -7.88995934, -37.2620163 , -17.66956329],\n",
       "       [ -7.83093262,  -8.66340637, -17.66956329, -33.12266541]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_distance = -dot_product_manual\n",
    "dot_product_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0b5483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although it might seem unusual for all the distances to be negative, the essential property of a distance metric is still preserved: smaller values indicate lower distance and thus greater similarity. So even with negative values, the relative comparisons remain valid—lower values still correspond to shorter distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24d5263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity and Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac0874e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual implementation of cosine similarity calculation¶\n",
    "#Since we’ve already covered how to compute the dot product, our strategy for manually calculating cosine similarity will focus on normalizing the vectors. This is because cosine similarity is simply the dot product of two normalized vectors, as was shown after the last equals sign in the cosine similarity calculation formula.\n",
    "\n",
    "#However, in order to normalize vectors, we must first compute their L2 norms.\n",
    "\n",
    "#Calculate the L2 norm\n",
    "#The following calculates the L2 norms for all the vectors in the embeddings array. The calculation simply squares each vector component, sums across columns (note the axis=1 parameter in the sum), and takes a square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b37b38e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.8089943, 6.2345276, 6.1042614, 5.7552285], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norms\n",
    "l2_norms = np.sqrt(np.sum(embeddings**2, axis=1))\n",
    "l2_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3fb8241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8089943],\n",
       "       [6.2345276],\n",
       "       [6.1042614],\n",
       "       [5.7552285]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norms reshaped\n",
    "l2_norms_reshaped = l2_norms.reshape(-1,1)\n",
    "l2_norms_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69a86f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embedding vectors\n",
    "\n",
    "#The following code calculates normalized embedding vectors by dividing every component in the vector by the vector's L2 norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35373f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03925694, -0.04243023, -0.00054956, ...,  0.07837522,\n",
       "         0.10917508,  0.09252975],\n",
       "       [-0.05740864, -0.05146182,  0.02560457, ..., -0.01130911,\n",
       "         0.14876871,  0.05514007],\n",
       "       [ 0.03326033, -0.0440653 ,  0.02667837, ..., -0.03219225,\n",
       "        -0.00553686,  0.09757371],\n",
       "       [-0.00740946, -0.07944359, -0.01655278, ..., -0.10083131,\n",
       "         0.02996998,  0.0158601 ]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings_manual = embeddings/l2_norms_reshaped\n",
    "normalized_embeddings_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56a51877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2 - Verify that vectors are normalized¶\n",
    "\n",
    "#Verify that normalized_embeddings_manual are normalized vectors by making sure that the length of each vector is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3045e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99999994, 1.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Note that the length of a vector can be found by taking its L2 norm. So, to solve this exercise, you just have to calculate the L2 norm of `normalized_embeddings_manual`, and verify that the sums are equal to or very close to 1:\n",
    "\n",
    "np.sqrt(np.sum(normalized_embeddings_manual**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6dd97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize embeddings using PyTorch\n",
    "#You can normalize embeddings in PyTorch using torch.nn.functional.normalize(). If your data is in a NumPy array, first convert it to a PyTorch tensor using torch.from_numpy(). After normalization, convert the tensor back to a NumPy array using the numpy() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37048bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03925694, -0.04243023, -0.00054956, ...,  0.07837522,\n",
       "         0.10917508,  0.09252975],\n",
       "       [-0.05740863, -0.05146182,  0.02560457, ..., -0.01130911,\n",
       "         0.1487687 ,  0.05514007],\n",
       "       [ 0.03326033, -0.0440653 ,  0.02667837, ..., -0.03219225,\n",
       "        -0.00553686,  0.09757371],\n",
       "       [-0.00740946, -0.07944359, -0.01655278, ..., -0.10083131,\n",
       "         0.02996998,  0.0158601 ]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings_torch = torch.nn.functional.normalize(\n",
    "    torch.from_numpy(embeddings)\n",
    ").numpy()\n",
    "normalized_embeddings_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09d1b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can verify that the normalized embeddings we calculated manually and the normalized embeddings calculated using torch are indeed identical using numpy's allclose() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0740b9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(normalized_embeddings_manual, normalized_embeddings_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22d1fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cosine similarity manually¶\n",
    "#To calculate cosine similarity between two normalized embedding vectors, we simply take their dot product. To do this, we can leverage the dot product function we defined before. For instance, the following calculates the cosine similarity between the vector embeddings of the first and second sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79089d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.5117967)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_fn(normalized_embeddings_manual[0], normalized_embeddings_manual[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b4d616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Likewise, to calculate the cosine similarities between all normalized vectors, we can use a nested loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea696f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000012, 0.51179671, 0.24167804, 0.23423393],\n",
       "       [0.51179671, 1.00000024, 0.20731851, 0.24144739],\n",
       "       [0.24167804, 0.20731851, 1.00000083, 0.50295591],\n",
       "       [0.23423393, 0.24144739, 0.50295591, 1.00000024]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_manual = np.empty([4,4])\n",
    "for i in range(normalized_embeddings_manual.shape[0]):\n",
    "    for j in range(normalized_embeddings_manual.shape[0]):\n",
    "        cosine_similarity_manual[i,j] = dot_product_fn(\n",
    "            normalized_embeddings_manual[i], \n",
    "            normalized_embeddings_manual[j]\n",
    "        )\n",
    "\n",
    "cosine_similarity_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9966454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity ranges from -1 to 1. The cosine similarity matrix is symmetric, just like the matrices for L2 distance and the dot product. In this example, cosine similarity performed well: the first two sentences were most similar to each other, as were the last two—matching our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3b5081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cosine similarity using matrix multiplication\n",
    "#Just like with the dot product, we can compute cosine similarity using matrix algebra. By multiplying the matrix of normalized embeddings with its transpose using the matrix multiplication operator, we obtain the cosine similarity matrix. This works because, once vectors are normalized, cosine similarity can be calculated by simply taking the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f62ce5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.5117967 , 0.24167815, 0.23423405],\n",
       "       [0.5117967 , 1.        , 0.20731854, 0.24144736],\n",
       "       [0.24167815, 0.20731854, 1.0000002 , 0.5029561 ],\n",
       "       [0.23423405, 0.24144736, 0.5029561 , 1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_operator = normalized_embeddings_manual @ normalized_embeddings_manual.T\n",
    "cosine_similarity_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d421e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can verify that the matrix algebra solution is the same as the one found using the nested loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52a328d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(cosine_similarity_manual, cosine_similarity_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41908e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19209290e-07,  4.88203287e-01,  7.58321956e-01,\n",
       "         7.65766069e-01],\n",
       "       [ 4.88203287e-01, -2.38418579e-07,  7.92681485e-01,\n",
       "         7.58552611e-01],\n",
       "       [ 7.58321956e-01,  7.92681485e-01, -8.34465027e-07,\n",
       "         4.97044086e-01],\n",
       "       [ 7.65766069e-01,  7.58552611e-01,  4.97044086e-01,\n",
       "        -2.38418579e-07]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate cosine distance¶\n",
    "\n",
    "#Using numpy, this can be calculated as follows:\n",
    "1 - cosine_similarity_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e06b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Similarity Search Using a Query\n",
    "\n",
    "# In the above examples, we calculated similarity between 4 documents:\n",
    "\n",
    "documents = [\n",
    "    'Bugs introduced by the intern had to be squashed by the lead developer.',\n",
    "    'Bugs found by the quality assurance engineer were difficult to debug.',\n",
    "    'Bugs are common throughout the warm summer months, according to the entomologist.',\n",
    "    'Bugs, in particular spiders, are extensively studied by arachnologists.'\n",
    "]\n",
    "\n",
    "#Now, your task is to find which of these 4 documents is most similar to the query Who is responsible for a coding project and fixing others' mistakes? using cosine similarity. You can reuse the documents and normalized_embeddings_manual arrays in your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abe2b306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bugs introduced by the intern had to be squashed by the lead developer.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, embed the query:\n",
    "query_embedding = model.encode(\n",
    "    [\"Who is responsible for a coding project and fixing others' mistakes?\"]\n",
    ")\n",
    "\n",
    "# Second, normalize the query embedding:\n",
    "normalized_query_embedding = torch.nn.functional.normalize(\n",
    "    torch.from_numpy(query_embedding)\n",
    ").numpy()\n",
    "\n",
    "# Third, calculate the cosine similarity between the documents and the query by using the dot product:\n",
    "cosine_similarity_q3 = normalized_embeddings_manual @ normalized_query_embedding.T\n",
    "\n",
    "# Fourth, find the position of the vector with the highest cosine similarity:\n",
    "highest_cossim_position = cosine_similarity_q3.argmax()\n",
    "\n",
    "# Fifth, find the document in that position in the `documents` array:\n",
    "documents[highest_cossim_position]\n",
    "\n",
    "# As you can see, the query retrieved the document `Bugs introduced by the intern had to be squashed by the lead developer.` which is what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9932844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
